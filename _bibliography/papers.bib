@string{aps = {American Physical Society,}}

@inproceedings{dicicco2023sigcomm, 
  bibtex_show={true},
  author = {Di Cicco, Nicola and Al Sadi, Amir and Grasselli, Chiara and Melis, Andrea and Antichi, Gianni and Tornatore, Massimo}, 
  title = {Poster: Continual Network Learning}, 
  year = {2023},
  isbn = {9798400702365}, 
  publisher = {Association for Computing Machinery}, 
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3603269.3610855}, 
  doi = {10.1145/3603269.3610855}, 
  abstract = {We make a case for in-network Continual Learning as a solution for seamless adaptation to evolving network conditions without forgetting past experiences. We propose implementing Active Learning-based selective data filtering in the data plane, allowing for data-efficient continual updates. We explore relevant challenges and propose future research directions.}, 
  booktitle = {Proceedings of the ACM SIGCOMM 2023 Conference},
  abbr={ACM SIGCOMM},
  pages = {1096-1098}, 
  numpages = {3}, 
  keywords = {continual learning, programmable data planes, in-network machine learning, active learning}, 
  location = {New York, NY, USA},
  series = {ACM SIGCOMM},
  pdf={sigcomm23.pdf}
}

@ARTICLE{dicicco2023deepls,
  bibtex_show={true},
  selected={true},
  author={Di Cicco, Nicola and Ibrahimi, Memedhe and Troia, Sebastian and Tornatore, Massimo},
  journal={IEEE Transactions on Network and Service Management}, 
  abbr={IEEE TNSM},
  title={DeepLS: Local Search for Network Optimization based on Lightweight Deep Reinforcement Learning}, 
  year={2023},
  pages={1-1},
  code={https://github.com/bonsai-lab-polimi/tnsm2023-deepls},
  pdf={tnsm23.pdf},
  doi={10.1109/TNSM.2023.3287433},
  abstract={Deep Reinforcement Learning (DRL) is being investigated as a competitive alternative to traditional techniques for solving network optimization problems. A promising research direction lies in enhancing traditional optimization algorithms by offloading low-level decisions to a DRL agent. In this study, we consider how to effectively employ DRL to improve the performance of Local Search algorithms, i.e., algorithms that, starting from a candidate solution, explore the solution space by iteratively applying local changes (i.e., moves), yielding the best solution found in the process. We propose a Local Search algorithm based on lightweight Deep Reinforcement Learning (DeepLS) that, given a neighborhood, queries a DRL agent for choosing a move, with the goal of achieving the best objective value in the long term. Our DRL agent, based on permutation-equivariant neural networks, is composed by less than a hundred parameters, requiring only up to ten minutes of training and can evaluate problem instances of arbitrary size, generalizing to networks and traffic distributions unseen during training. We evaluate DeepLS on two illustrative NP-Hard network routing problems, namely OSPF Weight Setting and Routing and Wavelength Assignment, training on a single small network only and evaluating on instances 2x-10x larger than training. Experimental results show that DeepLS outperforms existing DRL-based approaches from literature and attains competitive results with state-of-the-art metaheuristics, with computing times up to 8x smaller than the strongest algorithmic baselines.}
}

@INPROCEEDINGS{dicicco2023netsoft,
  abbr={IEEE NetSoft},
  selected={true},
  bibtex_show={true},
  author={Cicco, Nicola Di and Pittalà, Gaetano Francesco and Davoli, Gianluca and Borsatti, Davide and Cerroni, Walter and Raffaelli, Carla and Tornatore, Massimo},
  booktitle={2023 IEEE 9th International Conference on Network Softwarization (NetSoft)}, 
  title={DRL-FORCH: A Scalable Deep Reinforcement Learning-based Fog Computing Orchestrator}, 
  year={2023},
  pages={125-133},
  doi={10.1109/NetSoft57336.2023.10175398},
  code={https://github.com/bonsai-lab-polimi/netsoft2023-drl-forch},
  pdf={netsoft23.pdf},
  abstract={We consider the problem of designing and training a neural network-based orchestrator for fog computing service deployment. Our goal is to train an orchestrator able to optimize diversified and competing QoS requirements, such as blocking probability and service delay, while potentially supporting thousands of fog nodes. To cope with said challenges, we implement our neural orchestrator as a Deep Set (DS) network operating on sets of fog nodes, and we leverage Deep Reinforcement Learning (DRL) with invalid action masking to find an optimal trade-off between competing objectives. Illustrative numerical results show that our Deep Set-based policy generalizes well to problem sizes (i.e., in terms of numbers of fog nodes) up to two orders of magnitude larger than the ones seen during the training phase, outperforming both greedy heuristics and traditional Multi-Layer Perceptron (MLP)-based DRL. In addition, inference times of our DS-based policy are up to an order of magnitude faster than an MLP, allowing for excellent scalability and near real-time online decision-making.},
}

@INPROCEEDINGS{dicicco2023icc,
  bibtex_show={true},
  author={Di Cicco, Nicola and Talpini, Jacopo and Ibrahimi, Memedhe and Savi, Marco and Tornatore, Massimo},
  title={Uncertainty-Aware {QoT} Forecasting in Optical Networks with Bayesian Recurrent Neural Networks},
  booktitle={2023 IEEE International Conference on Communications (ICC): Optical Networks and Systems Symposium (IEEE ICC'23 - ONS Symposium)},
  abbr={IEEE ICC},
  year={2023},
  address={Rome, Italy},
  days={27},
  code={https://github.com/bonsai-lab-polimi/icc2023-qot-forecasting},
  pdf={icc23.pdf},
  abstract={We consider the problem of forecasting the Quality-of-Transmission (QoT) of deployed lightpaths in a Wavelength Division Multiplexing (WDM) optical network. QoT forecasting plays a determinant role in network management and planning, as it allows network operators to proactively plan maintenance or detect anomalies in a lightpath. To this end, we leverage Bayesian Recurrent Neural Networks for learning uncertainty-aware probabilistic QoT forecasts, i.e., for modelling a probability distribution of the QoT over a time horizon. We evaluate our proposed approach on the open-source Microsoft Wide Area Network (WAN) optical backbone dataset. Our illustrative numerical results show that our approach not only outperforms state-of-the-art models from literature, but also predicts intervals providing near-optimal empirical coverage. As such, we demonstrate that uncertainty-aware probabilistic modelling enables the application of QoT forecasting in risk-sensitive application scenarios.},
}

@INPROCEEDINGS{dicicco2023eucap,
  bibtex_show={true},
  author={Di Cicco, Nicola and Del Prete, Simone and Kodra, Silvi and Barbiroli, Marina and Fuschini, Franco and Vitucci, Enrico M. and Degli Esposti, Vittorio and Tornatore, Massimo},
  booktitle={2023 17th European Conference on Antennas and Propagation (EuCAP)},
  abbr={EuCAP}, 
  title={Machine Learning-Based Line-Of-Sight Prediction in Urban Manhattan-Like Environments}, 
  year={2023},
  pages={1-5},
  doi={10.23919/EuCAP57121.2023.10133145},
  pdf={eucap23.pdf},
  abstract={This paper considers the problem of predicting whether or not a transmitter and a receiver are in Line-of-Sight (LOS) condition. While this problem can be easily solved using a digital urban database and applying ray tracing, we consider the scenario in which only few high-level features descriptive of the propagation environment and of the radio link are available. LOS prediction is modelled as a binary classification Machine Learning problem, and a baseline classifier based on Gradient Boosting Decision Trees (GBDT) is proposed. A synthetic ray-tracing dataset of Manhattan-like topologies is generated for training and testing a GBDT classifier, and its generalization capabilities to both locations and environments unseen at training time are assessed. Results show that the GBDT model achieves good classification performance and provides accurate LOS probability modelling. By estimating feature importance, it can be concluded that the model learned simple decision rules that align with common sense.},
}

@INPROCEEDINGS{dicicco2022balkancom,
  bibtex_show={true},
  author={Di Cicco, Nicola and Ibrahimi, Mëmëdhe and Rottondi, Cristina and Tornatore, Massimo},
  booktitle={2022 International Balkan Conference on Communications and Networking (BalkanCom)},
  abbr={BalkanCom},  
  title={Calibrated Probabilistic QoT Regression for Unestablished Lightpaths in Optical Networks}, 
  year={2022},
  pages={21-25},
  doi={10.1109/BalkanCom55633.2022.9900791},
  code={https://github.com/bonsai-lab-polimi/balkancom2022-qot-regression},
  pdf={balkancom22.pdf},
  abstract={Quality-of-Transmission (QoT) regression of unestablished lightpaths is a fundamental problem in Machine Learning applied to optical networks. Even though this problem is well-investigated in current literature, many state-of-the-art approaches either predict point-estimates of the QoT or make simplifying assumptions about the QoT distribution. Because of this, during lightpath deployment, an operator might take either overly-aggressive or overly-conservative decisions due to biased predictions. In this paper, we leverage state-of-the-art Gradient Boosting Decision Tree (GBDT) models and recent advances in uncertainty calibration to perform QoT probabilistic regression for unestablished lightpaths. Calibration of a regression model allows for an accurate modeling of the QoT Cumulative Distribution Function (CDF) without any prior assumption on the QoT distribution. In our illustrative experimental results, we show that our calibrated GBDT model’s predictions provide accurate confidence interval estimates, even when only few samples per lightpath configuration are available at training time.},
}

@ARTICLE{dicicco2022jstqe,
  bibtex_show={true},
  selected={true},
  author={Di Cicco, Nicola and Mercan, Emre Furkan and Karandin, Oleg and Ayoub, Omran and Troia, Sebastian and Musumeci, Francesco and Tornatore, Massimo},
  journal={IEEE Journal of Selected Topics in Quantum Electronics}, 
  abbr={IEEE JSTQE}, 
  title={On Deep Reinforcement Learning for Static Routing and Wavelength Assignment}, 
  year={2022},
  volume={28},
  number={4: Mach. Learn. in Photon. Commun. and Meas. Syst.},
  pages={1-12},
  doi={10.1109/JSTQE.2022.3151323},
  pdf={jstqe22.pdf},
  abstract={Deep Reinforcement Learning (DRL) is rising as a promising tool for solving optimization problems in optical networks. Though studies employing DRL for solving static optimization problems in optical networks are appearing, assessing strengths and weaknesses of DRL with respect to state-of-the-art solution methods is still an open research question. In this work, we focus on Routing and Wavelength Assignment (RWA), a well-studied problem for which fast and scalable algorithms leading to better optimality gaps are always sought for. We develop two different DRL-based methods to assess the impact of different design choices on DRL performance. In addition, we propose a Multi-Start approach that can improve the average DRL performance, and we engineer a shaped reward that allows efficient learning in networks with high link capacities. With Multi-Start, DRL gets competitive results with respect to a state-of-the-art Genetic Algorithm with significant savings in computational times. Moreover, we assess the generalization capabilities of DRL to traffic matrices unseen during training, in terms of total connection requests and traffic distribution, showing that DRL can generalize on small to moderate deviations with respect to the training traffic matrices. Finally, we assess DRL scalability with respect to topology size and link capacity.},
}

@INPROCEEDINGS{dicicco2021drcn,
  bibtex_show={true},
  author={Di Cicco, Nicola and Cacchiani, Valentina and Raffaelli, Carla},
  booktitle={2021 17th International Conference on the Design of Reliable Communication Networks (DRCN)},
  abbr={DRCN}, 
  title={Scalable Multi-objective Optimization of Reliable Latency-constrained Optical Transport Networks}, 
  year={2021},
  pages={1-6},
  doi={10.1109/DRCN51631.2021.9477394},
  pdf={drcn21.pdf},
  abstract={In the evolving scenario of 5G end-to-end networks, optical transport networks provide the connectivity between the mobile edge and the mobile core network. According to the functional decoupling of the base station into the Remote Radio Unit (RRU) and the Baseband Unit (BBU), the latter can be virtualized into a cloud computing platform to access the mobile core. As a consequence, BBU virtual network functions related to different RRUs can be centralized and replicated in a subset of the nodes of the transport network with the aim of optimized reliable design.In this paper a scalable methodology, based on lexicographic optimization, is proposed for the solution of a multi-objective optimization problem to achieve, among other goals, the minimization of the number of active nodes in the transport network while supporting reliability and meeting latency constraints. The proposed solution method is compared to an aggregate optimization approach, showing that the former is capable of proving the optimality of the most relevant components of the multi-objective function (minimization of the number of active nodes and of the number of hops) for instances of medium size, and finds better solutions for instances with a larger number of nodes, namely several tens. The computing times to find an optimal solution for the most relevant objectives are much shorter than those required to solve the aggregate model, even for networks of several tens of nodes.},
}

@INPROCEEDINGS{quran2022ondm,
  bibtex_show={true},
  author={Quran, Abdullah and Troia, Sebastian and Ayoub, Omran and Di Cicco, Nicola and Tornatore, Massimo},
  booktitle={2022 International Conference on Optical Network Design and Modeling (ONDM)},
  abbr={ONDM},
  title={A Reinforcement Learning-based Dynamic Bandwidth Allocation for XGS-PON Networks}, 
  year={2022},
  pages={1-3},
  abstract={Time-division-multiplexing passive optical networks ({TDM}-{PONs}), with their massive deployment worldwide, are considered a fundamental technology for supporting not only traditional Internet broadband services, but also for new emerging 5G latency-sensitive services, such as Ultra-Reliable and Low Latency Communications (URLLC). Traditional dynamic bandwidth allocation (DBA) mechanisms, currently used to allocate network resources in {TDM}-{PONs}, are not suited to meet the requirements of these new services with strict latency requirements, as they use a polling mechanism which can result in a high queuing delay and ultimately violate URLLC latency requirements. In this work, we propose a new predictive-based DBA mechanism for Gigabit Symmetrical PON {(XGS-PON)} that allows to reduce the latency to fulfill requirements of emerging latency-sensitive services. Our solution employs reinforcement learning (RL) to predict the ingress buffer occupancy of {ONU}s in the next {DBA} cycle. Results show that the proposed {RL} method outperforms traditional DBA approaches in terms of upstream delay while maintaining similar frame loss ratio.},
  pdf={ondm22.pdf},
}

@article{ayoub22comnet,
  bibtex_show={true},
  abbr={ComNet},
  title = {Explainable Artificial Intelligence in communication networks: A use case for failure identification in microwave networks},
  journal = {Computer Networks},
  volume = {219},
  pages = {109466},
  year = {2022},
  issn = {1389-1286},
  doi = {https://doi.org/10.1016/j.comnet.2022.109466},
  url = {https://www.sciencedirect.com/science/article/pii/S138912862200500X},
  author = {Omran Ayoub and Nicola {Di Cicco} and Fatima Ezzeddine and Federica Bruschetta and Roberto Rubino and Massimo Nardecchia and Michele Milano and Francesco Musumeci and Claudio Passera and Massimo Tornatore},
  keywords = {Explainable artificial intelligence, Machine learning, Automated network management},
  pdf = {comnet22omran.pdf},
  abstract = {Artificial Intelligence (AI) has demonstrated superhuman capabilities in solving a significant number of tasks, leading to widespread industrial adoption. For in-field network-management application, AI-based solutions, however, have often risen skepticism among practitioners as their internal reasoning is not exposed and their decisions cannot be easily explained, preventing humans from trusting and even understanding them. To address this shortcoming, a new area in AI, called Explainable AI (XAI), is attracting the attention of both academic and industrial researchers. XAI is concerned with explaining and interpreting the internal reasoning and the outcome of AI-based models to achieve more trustable and practical deployment. In this work, we investigate the application of XAI for network management, focusing on the problem of automated failure-cause identification in microwave networks. We first introduce the concept of XAI, highlighting its advantages in the context of network management, and we discuss in detail the concept behind Shapley Additive Explanations (SHAP), the XAI framework considered in our analysis. Then, we propose a framework for a XAI-assisted ML-based automated failure-cause identification in microwave networks, spanning model’s development and deployment phases. For the development phase, we show how to exploit SHAP for feature selection and how to leverage SHAP to inspect misclassified instances during model’s development process, and how to describe model’s global behavior based on SHAP’s global explanations. For the deployment phase, we propose a framework based on predictions uncertainty to detect possibly wrong predictions that will be inspected through XAI.}
}

@article{dicicco2022comnet,
  bibtex_show={true},
  title = {Optimization over time of reliable 5G-RAN with network function migrations},
  journal = {Computer Networks},
  abbr={ComNet},
  volume = {215},
  pages = {109216},
  year = {2022},
  issn = {1389-1286},
  doi = {https://doi.org/10.1016/j.comnet.2022.109216},
  url = {https://www.sciencedirect.com/science/article/pii/S1389128622002985},
  author = {Nicola {Di Cicco} and Federico Tonini and Valentina Cacchiani and Carla Raffaelli},
  keywords = {Reliable 5G-RAN, Lexicographic optimization, Network function migrations},
  pdf={comnet22carla.pdf},
  abstract = {Resource optimization in 5G Radio Access Networks (5G-RAN) has to face the dynamics over time in networks with increasing numbers of nodes and virtual network functions. In this context, multiple objectives need to be jointly optimized, and key application requirements such as latency must be enforced. In addition, virtual network functions realizing baseband processing are subject to failures of the cloud infrastructure, requiring an additional level of reliability. Overall, this is a complex problem to solve, requiring fast algorithms to cope with dynamic networks while avoiding resource overprovisioning. This paper considers the problem of optimal virtual function placement in 5G-RAN with reliability against a single DU Hotel failure and proposes a solution that takes service dynamics into account. Firstly, the joint optimization of the total number of DU Hotels, of the RU–DU latency and of the backup DU sharing in a static traffic scenario is considered, and the DUOpt algorithm, based on Lexicographic Optimization, is proposed for solving efficiently this multi-objective problem. DUOpt splits the multi-objective problem into smaller Integer Linear Programming (ILP) subproblems that are sequentially solved, adopting for each one the most effective methodology to reduce the total execution time. The proposed DUOpt algorithm is extensively benchmarked to show its effectiveness in optimization of medium to large size networks: in particular, it is shown to greatly outperform an aggregate multi-objective approach, being able to compute optimal or close to optimal solutions for networks of several tens of nodes in computing times of a few seconds. Then, the problem is extended to a dynamic traffic scenario in which optimization is performed over time. In this context, in addition to the aforementioned objectives, the total number of network function migrations induced by multiple reoptimizations must be kept to the minimum. For solving efficiently this problem the DUMig algorithm is proposed, which extends and improves DUOpt. Reoptimization over a time horizon of one day in an illustrative dynamic traffic scenario is performed to evaluate the proposed DUMig algorithm against DUOpt, the latter being oblivious of the traffic dynamics. DUMig shows remarkable savings in the total number of migrations (above 86.1\% for primary virtual functions and 83\% for backup virtual functions) compared to DUOpt, while preserving near-optimal resource assignment.}
}